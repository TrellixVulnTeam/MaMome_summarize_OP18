Zhou and Gallins

Host Trait Prediction

levels of taxonomy, with intriguing thoughts about the
of
interplay and use of molecular function descriptors vs. taxonomic
descriptors, the reader is referred to Knights et al. and
Xu et al.  However, many of the principles discussed here
apply regardless of the feature type.

Several features of OTU tables present challenges. First, OTU
tables are sparse, with a large proportion of zero counts (Hu
et al., 2018). Investigators have often removed OTUs that were
present in too few samples to be useful, or collapsed OTUs into
the genus level, which is a simple form of “feature engineering”
that we will explore further below. Second, the role of taxonomy
in prediction is often unclear – similar sequences are often
correlated across samples, which is a property that can be readily
assessed directly without taxonomic knowledge. Third, as with
many omics technologies, library sizes (essentially column sums
of the OTU table) vary considerably, and normalization methods
must be used to account for this variation (Weiss et al., 2017).

A number of excellent reviews have been published, covering
experimental design and targeted amplicon vs. metagenomics
proﬁling (Mallick et al., 2017), and a comprehensive overview of
diﬀerent experimental and interrogation methods and analyses
(Knight et al., 2018). Other reviews have covered the remarkable
advances in understanding that have resulted recently in
understanding connections of, e.g., human gut microbiome
populations to human health (Cani, 2018).

Recently, studies have begun to explore the power of
machine learning to use microbiome patterns to predict host
characteristics (Knights et al., 2011a; Moitinho-Silva et al.,
2017). Existing studies often report disease-associated dysbiosis,
a microbial imbalance inside the host, but such associations can
have a wide range of interpretations. Individual studies have
also suﬀered from small sample sizes, inconsistent ﬁndings, and
a lack of standard processing and analysis methods (Duvallet
et al., 2017). Prediction models have sometimes been diﬃcult to
generalize across studies (Pasolli et al., 2016). One approach to
resolve these issues is by performing a meta-analysis, combining
microbiome studies across common traits. Duvallet et al. 
have performed a cross-disease meta-analysis of published
case-control gut microbiome studies spanning 10 diseases.
They found consistent patterns characterizing disease-associated
microbiome changes and concluded that many associations
found in case-control studies are likely not disease-speciﬁc
but rather part of a non-speciﬁc, shared response to health
and disease. Pasolli et al. also performed a meta-
analysis in a collection of 2,424 publicly available samples from
eight large-scale studies. The authors remarked that addition
of healthy (control) samples from other studies to training
sets improved disease prediction capabilities. Nonetheless, any
meta- or pooled analysis should rely on a solid foundation
of eﬀective per-study prediction. The use of multiple studies
enabled Pasolli et al. to explore the use of external
validation of models across truly separate datasets. Such
external validation can in principle result
in more robust
and generalizable models for prediction than models that are
validated internally only.

Sophisticated machine learning methods in microbiome
analysis have been proposed considerably in recent years,

including using deep neural networks (Ananthakrishnan et al.,
2017), and leveraging methods for genomes and metagenomes
(Rahman et al., 2018). However, the content-knowledge required
to implement these methods is high, presenting a barrier to
data scientists looking to get started in microbiome analysis and
prediction. Moreover, there are few resources for biologists with
intermediate statistical and computing background to “jump in”
to analysis of the important trait prediction problem. The target
audience of this paper is those seeking a brief review and tutorial
for trait prediction, and who will beneﬁt from accessible code.
After digesting these basic building blocks of analysis, the reader
may move to more advanced, such as dynamic systems modeling
(Brooks et al., 2017).

The remainder of this paper is written in several sections.
Section 2 reviews
the steps of data preparation before
machine learning implementation. Section 3 provides a quick
overview of the most commonly-used machine learning (ML)
methods, as well as the most commonly used performance
criteria. Experienced modelers can skip this section. Section 4
summarizes the scope of the relevant literature and describes
several real datasets and the trait of interest. Section 5 provides
results, and the underlying code forms a tutorial of machine
learning methods applied in this context.

2. DATA PREPARATION

Many machine learning methods have diﬃculty with missing
features, and so we assume the OTU table is complete.
A minor fraction of missing data can often be eﬀectively
handled using simple imputation procedures, such as kNN-
impute (Crookston and Finley, 2008), or even simpler methods,
such as feature-median imputation. The methods described in
this section, including imputation and normalization, must be
performed without using the host trait information, because
otherwise they might be biased by this information. Feature
selection methods that use host
information belong
in the next section, as they must be included inside a
cross-validation procedure.

trait

2.1. Notation and Sampling Considerations
Let X be an m × n matrix of microbiome count data, where
m is the number of OTU features and n is the number of
samples. Let y be a vector of length n with the microbiome
host trait. Commonly a trait will be a binary outcome (e.g.,
case/control status, coded 1/0), or a continuous trait, such as
body mass index (BMI). Here our use of microbiome features
as predictive of a trait does not imply or assume causality. We
note that case/control study designs often involve oversampling
of one type (often cases) relative to the general population. A
prediction rule might explicitly use this information, for example
by a simple application of Bayes’ rule (Tibshirani et al., 2003),
with prior probabilities reﬂecting those in the general population.
Such sampling considerations are beyond our scope, and we
refer the reader to Chawla  Here we consider our sample
dataset to be representative of the population of its intended
downstream use.

Frontiers in Genetics | www.frontiersin.org

2

June 2019 | Volume 10 | Article 579

Zhou and Gallins

Host Trait Prediction

2.2. Transformation and Normalization
Normalization is an essential process to ensure comparability of
data across samples (Weiss et al., 2017), largely to account for
the large variability in library sizes (total number of sequencing
reads across diﬀerent samples). The basic issues are similar
to those encountered in expression sequence normalization
(de Kok et al., 2005), but
less is currently known about
sources of potential bias to inform microbiome normalization.
Normalization methods assessed by Weiss et al. included
cumulative sum scaling, variance stabilization, and trimmed-
mean by M-values. Randolph et al. utilized the centered
log-ratio (CLR) transform of the relative abundance vectors,
based on a method developed by Aitchison  replacing
zeros with a small positive value. As part of their motivation,
Randolph et al. pointed out that standard cumulative
sum scaling places the normalized data vectors in a simplex,
with potential consequences for kernel-based discovery methods
(Randolph et al., 2018).

2.3. Taxonomy as Annotation
Taxonomy is the science of deﬁning and naming groups of
biological organisms on the basis of shared characteristics. In
our context, taxonomy refers to the evolutionary relationship
among the microbes represented by each OTU, from general
to speciﬁc: kingdom, phylum, class, order, family, genus and
species, and OTU (Oudah and Henschel, 2018). For example,
Kostic et al. summarized their ﬁndings in the study
of microbiota in colorectal cancer using genera and phyla-
level summaries, illustrating the importance of taxonomy in
interpretation. Here we are highlighting the use of taxonomy
in post-hoc interpretation of ﬁndings, providing important
biological context. However,
if the taxonomy is used in a
supervised manner to improve prediction, it then becomes part
of the formal machine learning procedure, as described in the
next section.

3. REVIEW OF MACHINE LEARNING
METHODS FOR PREDICTION

in the data, but without

Machine learning deals with the creation and evaluation of
algorithms to recognize, classify, and predict patterns from data
(Tarca et al., 2007). Unsupervised methods identify patterns
apparent
the use of pre-deﬁned
in our context). These methods include (i)
labels (traits,
hierarchical clustering, which builds a hierarchy of clusters
using a dendrogram, combining or splitting clusters based on a
measure of dissimilarity between vectors of X; and (ii) k-means
clustering, which involves partitioning the n vectors of X into
k clusters in which each observation is classiﬁed to a cluster
mean according to a distance metric. Unsupervised methods are
important exploratory tools to examine the data and to determine
important data structures and correlation patterns.

For the host trait prediction problem, we focus on supervised
methods, in which labels (traits) of a dataset are known, and
we wish to train a model to recognize feature characteristics
associated with the trait. A primary diﬃculty in the problem is

that the number of features (m rows) in the OTU table may
greatly exceed the sample size n, so that over-ﬁtting of complex
models to the data is a concern.

3.1. Training and Cross-Validation
Training a model in supervised learning amounts to ﬁnding a
parameter vector β that represents a rule for predicting a trait
y from an m-vector x. This rule may take the form of a regression
equation or other prediction rule. Prediction rules that use only
a few features (n or fewer) are referred to as “sparse.” A good
prediction rule has high accuracy, as measured by quantities, such
as the area under the receiver-operator characteristic curve, or the
prediction correlation R, both described below. Many prediction
methods proceed by minimizing an objective function obj(β) =
L(β) + (cid:127)(β), which contains two parts: the raw training loss
L and a regularization term (cid:127). The training loss measures how
predictive the model is with respect to the data used to train the
model, and the regularization term penalizes for the complexity
of the model, which helps to avoid overﬁtting.

An essential component of machine learning is the use of
cross-validation to evaluate prediction performance, and often to
select tuning parameters that govern the complexity of the model.
One round of k-fold cross-validation involves partitioning the n
samples into k subsets of roughly equal size, using each subset
in turn as as the validation data for testing the algorithm, with
the remaining samples as the training set. After a single round
of cross-validation, each sample i has an associated predicted
trait value ˆyi, where the prediction rule was developed without
any knowledge of the data from sample i (or at least without
knowledge of yi). The performance measure is computed by
comparing the length-n ˆy vector to the true y. To reduce
variability, multiple rounds of cross-validation are performed
using diﬀerent partitions, and the validation results are averaged
over the rounds to give an estimate of the predictive performance.
Although the term “cross-validation” formally refers to the use of
each sample i as both part of the training set and as testing set
(i.e., crossing) during a single round, the term is often used more
generically. For example, researchers sometimes use a simple
holdout method in which a fraction 1/k of the data are randomly
selected as a test set, the remainder as training, and repeat
the process randomly with enough rounds to provide a stable
estimate of accuracy.

3.2. Taxonomy and Structural Feature
Extraction
Our Results section shows the results of prediction methods using
all OTUs, as well as reduced-OTU selected or aggregated features.
Several methods have been proposed to reduce the number
of OTU features using correlation and taxonomy information,
including Fizzy (Ditzler et al., 2015a), MetAML (Pasolli et al.,
2016), and HFE (Oudah and Henschel, 2018). Aspects of the
approaches are supervised and thus must be handled inside a
cross-validation procedure.

For simplicity, here we focus on the hierarchical feature
engineering (HFE) algorithm created by Oudah and Henschel
 which uses correlation and taxonomy information in X to
exploit the underlying hierarchical structure of the feature space.

Frontiers in Genetics | www.frontiersin.org

3

June 2019 | Volume 10 | Article 579

Zhou and Gallins

Host Trait Prediction

The HFE algorithm consists of four steps: feature engineering:
consider the relative abundances of higher taxonomic units as
potential features by summing up the relative abundances of their
respective children in a bottom-up tree traversal; correlation-
based ﬁltering: calculate the correlation of values for each parent-
child pair in the taxonomy hierarchy, and if the result is greater
than a predeﬁned threshold, then the child node is discarded;
based ﬁltering, reﬂecting association
of features to the trait: construct all paths from the leaves (OTUs)
to the root and for each path, calculate the IG of each node with
respect to the trait values, and then calculate and use the average
IG as a threshold to discard any node with a lower IG score;
IG-based leaf ﬁltering: for OTUs with incomplete taxonomic
information, discard any leaf with an IG score less than the
global average IG score of the remaining nodes from the third
phase. Steps must be cross-validated, as they use the
trait values. The python code for implementation is on our site
(https://sites.google.com/ncsu.edu/zhouslab/home/software?).

The result is a set of informative features, perhaps including
original OTUs along with higher-level aggregations of taxonomic
features, that can be utilized for downstream machine learning
(Oudah and Henschel, 2018). Standard feature selection
algorithms, Fizzy and MetAML, which do not capitalize on the
hierarchical structure of features, were also tested by Oudah
and Henschel using several machine learning methods
on real datasets. Since HFE was reported to outperform other
methods (Oudah and Henschel, 2018) and resulted in higher
prediction performance overall, we apply it in the real data
analysis section to extract OTU features before applying machine
learning methods of trait prediction. Note that feature selection
can in principle be performed inside a grand cross-validation and
prediction loop, or performed prior to prediction, as we have
done for convenience here.

3.3. Supervised Learning Methods
Commonly Used in Trait Prediction
Here we list the learning methods most commonly used in
microbiome host trait prediction. The list is not exhaustive, but
reﬂects our review of the methods in common use. In particular,
neural networks have received considerable recent attention, but
it is diﬃcult to ﬁnd quantitative evidence for the additional
predictive ability in comparison to other methods. For several
of the methods, it is common to center and row-scale X prior
to application of the method, so each feature is given similar
“weight” in the analysis.

3.3.1. Regression
The use of linear models enables simple ﬁtting of continuous
traits y as a function of feature vectors. However, if m ≥ n then
structural overﬁtting occurs, and even if m < n accuracy is often
improved by using penalized (regularized) models. For the model
y = Xβ + ǫ, the training loss is Pi(yi − ˆyi)2 the most commonly-
used regularization methods are ridge regression (Hoerl and
Kennard, 1970) and Lasso (Tibshirani, 1996) regression, which
respectively use penalties λ Pi β2
i and λ Pi |βi| (not including
the intercept) to the training loss. For binary class prediction,
the approach is essentially the same, applying a generalized linear

(logit) model, with the negative log-likelihood as the training loss.
Here λ is a tuning parameter that can be optimized as part of
cross-validation. Both methods provide “shrunken” coeﬃcients,
i.e., closer to zero than an ordinary least-squares approach. The
results for Lasso are also sparse, with no more than n non-zero
coeﬃcients after optimization, and thus Lasso is also a feature-
selection method. Another variant is the elastic net (Zou and
Hastie, 2005), an intermediate version that linearly combines
both penalties.

3.3.2. Linear Discriminant Analysis (LDA)
For binary traits, this approach ﬁnds a linear combination of
OTUs in the training data that models the multivariate mean
diﬀerences between classes (Lachenbruch and Goldstein, 1979).
Classical LDA assumes that feature data arise from two diﬀerent
multivariate normal densities according to y = 0 and y = 1,
i.e., MVN(µ0, 6) and MVN(µ1, 6) (Figure 1A). The prediction
value is the estimate of the posterior mean E(Y|x) = Pr(Y =
1|X), used because it minimizes mean-squared error.

3.3.3. Support Vector Machines (SVM)
This is another approach in the linear classiﬁer category
(Figure 1A), but in contrast to LDA may be considered non-
parametric. In SVM, the goal is to ﬁnd the hyperplane in a high-
dimensional space that represents the largest margin between
any two instances (support vectors) of two classes of training-
data points, or that maximizes a related function if they cannot
be separated. Non-linear versions of SVM are devised using a
so-called kernel similarity function (Cortes and Vapnik, 1995).

3.3.4. Similarity Matrices and Related Kernel Methods
Some applications of microbiome association testing have
compared similarity matrices across features to similarity of
traits (Zhao and Shojaie, 2016). A closely-related approach is to
ﬁrst compute principal component (PC) scores, which may be
obtained from OTU sample-sample correlation matrices (Zhou
et al., 2018), and to use these PC scores as trait predictors.
Kernel-penalized regression, an extension of PCA, was utilized by
Randolph et al.  in their microbiome data analysis. They
applied a signiﬁcance test for their graph-constrained estimation
method, called Grace (Zhao and Shojaie, 2016), to test for
association between microbiome species and their trait. However,
trait prediction is not available in their software.

3.3.5. k-Nearest Neighbors (k-NN)
Training samples are vectors in a multi-dimensional space, each
with a class label or continuous trait value. For discrete traits, a
test sample is assigned the label which is most frequent among the
k training samples nearest to that point (Figure 1B). Euclidean
distance or correlation coeﬃcients are the most commonly used
distance metrics. For continuous traits, a weighted average of
the k nearest neighbors is used, sometimes weighted (e.g., by the
inverse of their distance from the new point).

3.3.6. Random Forests
Random forests (Breiman, 2001) are an increasingly used
method, extensively applied in many diﬀerent ﬁelds, including
computational biology and genomics (Statnikov et al., 2013)

Frontiers in Genetics | www.frontiersin.org

4

June 2019 | Volume 10 | Article 579

Zhou and Gallins

Host Trait Prediction

FIGURE 1 | Schematic illustration of several machine learning prediction methods using case/control (red/blue) status. For two features, (A) illustrates linear
discrimination methods. The solid line shows the linear discriminant line corresponding to equally probable outcomes, while the dashed line shows the midpoint of the
maximum-margin support vector machine. (B) For k-nearest neighbors, the gray point is predicted using an average of the neighbors (red, in this instance). (C)
Decision tree ensembles include random forests, which average over bootstrapped trees, and boosted trees, where successive residuals are used for ﬁtting. Trees may
not extend to the level of individual observations, and modal or mean values in the terminal nodes are used for prediction. (D) A neural network with few hidden layers.

The building block of a “forest” is a decision tree, which
uses features and associated threshold values to successively
split the samples into groups that have similar y values. This
process is repeated until the total number of speciﬁed nodes
is reached. An ensemble of decision trees (or regression trees
for continuous y) is built by performing bootstrapping on the
dataset and averaging or taking the modal prediction from trees
(a process known as “bagging”)(Figure 1C), with subsampling
of features used to reduce generalization error (Ho, 1995).
An ancillary outcome of the bootstrapping procedure is that
the data not sampled in each bootstrap (called “out of bag”)
can be used to estimate generalization error, as an alternative
to cross-validation.

3.3.7. Gradient Boosting
Gradient boosting for decision trees refers to a process of
ensemble modeling by averaging predictions over decision trees
(learners) of ﬁxed size (Friedman, 2001). As with other forms
of boosting, the process successively computes weights for the
individual learners in order to improve performance for the
poorly-predicted samples. Following observations that boosting
can be interpreted as a form of gradient descent on a loss function
(such as Pi(yi − ˆyi)2), gradient tree boosting successively

ﬁts decision trees on quantities known as “pseudo-residuals”
(Friedman, 2002) for the loss function (Figure 1C).

refer

3.3.8. Neural Networks
Neural networks
to an interconnected feed-forward
network of nodes (“neurons”) with weights attached to each
edge in the network, which allows the network to form a
mapping between the inputs X and the outcomes y (Ditzler
et al., 2015a). Each neuron j receiving an input pj(t) from
predecessor neurons consists of the following components: an
activation aj(t), a threshold θj, an activation function f
that
computes the new activation at a given time t + 1, and an output
function fout computing the output from the activation. These
networks contain either one or many hidden layers, depending
on the network type (Figure 1D). For microbiome data, the input
layer is the set of OTUs, with separate neurons for each OTU.
Hidden layers use backpropagation to optimize the weights of
the input variables in order to improve the predictive power of
the model. The total number of hidden layers and number of
neurons within each hidden layer are speciﬁed by the user. All
neurons from the input layer are connected to all neurons in
the ﬁrst hidden layer, with weights representing each connection.
This process continues until the last hidden layer is connected

Frontiers in Genetics | www.frontiersin.org

5

June 2019 | Volume 10 | Article 579

Zhou and Gallins

Host Trait Prediction

TABLE 1 | Review of published prediction accuracy comparisons.

Paper

Dataset

Trait

Samples Cases Controls

Taxa

Level

Method

Metric

Value

Pasolli et al., 2016 Qin et al., 2014

Liver cirrhosis

232

118

114

542

Species Random forest

Zeller et al., 2014 Colorectal cancer

121

48

73

503

Species Random forest

Qin et al., 2010

IBD

110

25

85

443

Species Random forest

Le Chatelier et al.,
2013

Obesity

253

164

89

465

Species Random forest

Qin et al., 2012

Type II diabetes

344

170

174

572

Species Random forest

Karlsson et al.,
2013

Type II diabetes

96

53

43

381

Species Random forest

SVM

Elastic net

Lasso

SVM

Elastic net

Lasso

SVM

Elastic net

Lasso

SVM

Elastic net

Lasso

SVM

Elastic net

Lasso

SVM

Elastic net

Lasso

Johnson et al.,
2016

Post-mortem
interval (PMI)

67

NA

NA

52

Phylum Ridge

Ditzler et al.,
2015b

Soil pH
(low/medium/high)

Rousk, 2010

22

NA

NA

500

Various

52

Phylum Elastic net

3,130

Species

Lasso

52

Phylum SVM

3,130

Species Ridge

3,130

Species Elastic net

52

Phylum Lasso

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

AUC

Error rate

Error rate

Error rate

Error rate

Error rate

Error rate

Error rate

Error rate

0.95

0.92

0.91

0.88

0.87

0.81

0.79

0.73

0.89

0.86

0.83

0.81

0.66

0.65

0.64

0.60

0.74

0.66

0.70

0.71

0.76

0.66

0.60

0.54

0.46

0.48

0.49

0.50

0.51

0.52

0.52

0.15

Caporaso et al.,
2011

Host gender

1,967

NA

NA

500

various

Error rate

0.15

Recursive neural network
(RNN)


Deep belief network (DBN)


Deep belief network (DBN)


Random forest

Multi-layer perceptron
Neural network (MLPNN)


Recursive neural network
(RNN) 

Recursive neural network
(RNN) 

Deep belief network (DBN)


Deep belief network (DBN)


Error rate

0.08

Error rate

0.08

Error rate

Error rate

0.15

0.00

Error rate

0.19

Error rate

0.24

Error rate

0.24

(Continued)

Frontiers in Genetics | www.frontiersin.org

6

June 2019 | Volume 10 | Article 579

Zhou and Gallins

TABLE 1 | Continued

Paper

Dataset

Trait

Samples Cases Controls

Taxa

Level

Method

Metric

Value

Caporaso et al.,
2011

Three body sites

1,967

NA

NA

500

Various

Error rate

0.17

Reiman et al.,
2017

Caporaso et al.,
2011

Three body sites

1,967

NA

NA

1,706

Various

Accuracy

0.83

Moitinho-Silva
et al., 2017

Microbial
abundance from
sponges (high/low)

1,232

NA

NA

30

Phylum random forest

Accuracy

0.97

76

Class

Random forest

2,322

Various

Random forest

Ai et al., 2017

141

42

99

1,171

Species Bayes net

Colorectal cancer
(CRC)

141

53

88

783

Species Bayes net

Wu et al., 2018

Three diseases

806

423

383

300

Genus

Logistic

Frontiers in Genetics | www.frontiersin.org

7

June 2019 | Volume 10 | Article 579

Host Trait Prediction

Error rate

Error rate

0.03

0.08

Error rate

0.16

Error rate

0.03

Error rate

0.03

Error rate

Error rate

0.01

0.01

Accuracy

0.84

Accuracy

0.97

Accuracy

0.97

Accuracy

0.99

Accuracy

Accuracy

0.99

0.95

Accuracy

0.99

Accuracy

0.95

Accuracy

Accuracy

Accuracy

Accuracy

AUC

AUC

AUC

AUC

AUC

AUC

F1

F1

F1

F1

0.95

0.91

0.50

0.91

0.93

0.94

0.98

0.86

0.86

0.71

0.91

0.86

0.83

0.91

(Continued)

Random forest

Multi-layer perceptron
neural network (MLPNN)


Recursive neural network
(RNN) 

Recursive neural network
(RNN) 

Deep belief network (DBN)


Deep belief network (DBN)


Random forest

Multi-layer perceptron
neural network (MLPNN)


Recursive neural network
(RNN) 

Recursive neural network
(RNN) 

Deep belief network (DBN)


Deep belief network (DBN)


Multi-layer perceptron
Neural network (MLPNN)


Random forest

Convolutional neural
Network (CNN-1D)

Convolutional neural
Network (CNN-2D)

Adaptive boosting
(AdaBoost)

Adaptive boosting
(AdaBoost)

Adaptive boosting
(AdaBoost)

Random forest

Logistic

Random forest

Logistic

k-nearest neighbor

Random forest

SVM

Zhou and Gallins

TABLE 1 | Continued

Host Trait Prediction

Paper

Dataset

Trait

Samples Cases Controls

Taxa

Level

Method

Metric

Value

Nakano et al.,
2018

Oral malodour

90

45

45

37

Genus

Asgari et al., 2018 HMP

Five body sites

1,192

NA

NA

20,589 Various

Random forest

Gevers et al., 2014 Crohn’s disease

1,359

731

628

9,511

Various

Random forest

Gradient boosting

Adaptive boosting

SVM
Deep learning

SVM

SVM

Accuracy
Accuracy

F1

F1

F1

F1

F1

F1

0.87

0.90

0.79
0.97

0.89

0.85

0.74

0.68

FIGURE 2 | (A–C) ROC curves for each machine learning method using all OTUs. The AUC values are shown in the legend. The size of each dataset (#
cases/controls ×# OTUs) is shown in the title. (D) Bar graph showing the average Pearson correlation (R) between predicted and actual BMI in the Goodrich dataset,
using BMI as a continuous trait.

to the output layer. A bias term is also added in each step,
which can be thought of as analogous to the intercept of a
linear model. The output layer are predictions based on the
data from the input and hidden layers. In most cases, having
just one hidden layer with one neuron is reasonable to ﬁt
the model.

ˆ
y of binary traits,

3.4. Measures of Prediction Accuracy: The
AUC and Prediction R
For predictions
the receiver operating
characteristic (ROC) curve plots the true positive rate (TPR)
against the false positive rate (FPR) at various threshold settings.
The true-positive rate is also known as sensitivity, or a probability
of detection. The area under the ROC curve (AUC) is the
most common measure of prediction accuracy for binary traits,

and ranges from 0.5 (no better than chance) to 1.0 (perfect
discrimination). In practice, the empirical AUC can be <0.5,
the prediction procedure
in which case we conclude that
has no value. Note that the AUC is invariant to monotone
transformations of ˆy.

The prediction Pearson correlation (R) between cross-
is a commonly-
validated predicted and actual y values
used standard of accuracy for continuous traits, although
many procedures are designed to minimize the mean-squared
prediction error Pi(yi − ˆyi)2. R ≤ 0 corresponds to no predictive
value, and R = 1 to perfect prediction. We advocate R as a
criterion because it is simple and applicable to many prediction
procedures. Some prediction procedures may have an oﬀset or
proportional bias in prediction that may harm the mean-squared
error, even if R is favorable. A post-hoc linear rescaling of the

Frontiers in Genetics | www.frontiersin.org

8

June 2019 | Volume 10 | Article 579

Zhou and Gallins

Host Trait Prediction

prediction to “ﬁx” any such bias is straightforward, and we ﬁnd it
simplest to directly use R for comparison.

In the real data analyses below, the predicted ˆy represent
average predictions over all cross-validation rounds, so the AUC
and R values were computed directly on the resulting predictions.
Importantly, the use of cross-validation provides for each dataset
a measure of actual performance of a prediction method, without
relying on theoretical considerations, simulations, or restrictive
assumptions that may not be applicable with real data.

4. DATA USED FOR COMPARISONS

4.1. A Literature Review
We conducted a literature review of published host-trait
microbiome prediction studies that used cross-validation and
reported a measure of prediction accuracy. We conducted a
literature review of published host-trait microbiome prediction
studies that used cross-validation and reported a measure of
prediction accuracy. A full table appears in the Supplement,
including links to each of the 18 studies with 54 reported datasets
represented. As diﬀerent studies used vastly diﬀerent protocols
for OTU generation and preprocessing, for this main paper
we focused on the 17 reported datasets that compared at least
two competing measures of prediction accuracy. As diﬀerent

studies used vastly diﬀerent protocols for OTU generation and
preprocessing, for this main paper we focused on the 17 reported
datasets that compared at least two competing measures of
prediction accuracy. All of the datasets were using human hosts,
except for Rousk et al. (where pH in soil samples was
the “trait”) and Moitinho-Silva et al.  where microbial
abundance in sponges was the trait.

evaluated

supervised

addition, we

4.2. Analyses of Data Using Competing
Methods
learning
In
the
methods ourselves using datasets
from MicrobiomeHD
(https://github.com/cduvallet/microbiomeHD), a standardized
database of human gut microbiome studies in health and
disease. This database includes publicly available 16S rRNA
data from published case-control and other studies and their
associated patient metadata. The MicrobiomeHD database
these datasets are
and original publications
described in Duvallet et al.  Raw sequencing data
for each study was downloaded and processed through a
standardized pipeline.

for each of

For our analyses, we analyzed four traits (three binary and
one continuous) from three datasets with varying sample sizes

FIGURE 3 | ROC curves after collapsing OTUs to the genus level (A) the Singh dataset, (B) the Vincent dataset, and (C) the Goodrich dataset.

Frontiers in Genetics | www.frontiersin.org

9

June 2019 | Volume 10 | Article 579

Zhou and Gallins

Host Trait Prediction

and initial numbers of OTUs: data
set, containing 201 EDD (enteric diarrheal disease) cases vs.
82 healthy controls with 1, 325 OTUs. The Vincent et al.

cases vs. 25 healthy controls and 763 OTUs. The Goodrich
et al. dataset, which categorized the hosts into 135 obese
cases vs. 279 controls, based on body mass index (BMI), with
a total of 11, 225 OTUs. In this dataset, individuals came from
the TwinsUK population, so we included only one individual
from each twin-pair. 
dataset, but using BMI directly as a continuous phenotype for the
same 414 individuals. The microbiome samples for each dataset
were obtained from stool, and we analyzed one sample per
individual throughout.

Following the ﬁltering recommendations applied by Duvallet
et al.  we removed samples with fewer than 100 reads
and OTUs with fewer than 10 reads. We also removed OTUs
which were present in <1% of samples from the Vincent et al.
datasets, and
<5% of samples from the Goodrich et al. datasets, since

it contained many more OTUs. Then we scaled the datasets by
calculating the relative abundance of each OTU, dividing its value
by the total reads per sample.

In our primary analysis, we tested the relative abundances
of the microbiome data at the OTU level. We also ran analyses
in which OTUs were collapsed to the genus level by summing
their respective relative abundances, discarding any OTUs which
were un-annotated at the genus level. Finally, we ran the
hierarchical feature engineering (HFE) algorithm introduced by
Oudah and Henschel which results fewer informative
features, including individual OTUs and aggregated elements of
the taxonomy.

We performed 100 rounds of 5-fold cross-validation for each
supervised method, using diﬀerent random splits for each round.
For binary traits, the estimated group probability ˆP(Y = 1|X)
was used to estimate the group assignment. These estimates were
further averaged over the cross-validation rounds. Performance
was evaluated using the AUC. For continuous traits, the direct
estimate ˆy was used, averaged over cross-validations, with
performance criterion R.

FIGURE 4 | ROC curves after applying the HFE method to select a subset of informative features (A) Singh dataset, (B) Vincent dataset, (C) Goodrich dataset.

Frontiers in Genetics | www.frontiersin.org

10

June 2019 | Volume 10 | Article 579

Zhou and Gallins

Host Trait Prediction

R code for the comparisons is available at https://sites.
google.com/ncsu.edu/zhouslab/home/software?, and here we list
the packages and settings used. Five-fold cross-validation was
used throughout, and we additionally checked for plausibility.
For example,
the out-of-bag accuracy estimates from the
random forest procedure were compared to our cross-validated
estimates and shown to match closely. All machine learning
methods were used for each dataset as applicable (for example,
LDA was applicable only for the discrete trait datasets).
All predictions used probability estimates for the discrete
traits. The random forest method used randomForest
with ntree=500, mtry=sqrt(ncol(X)). The gradient
boosting (Gboost) decision-tree approach used xgboost, with
nrounds=10 and objective= “binary:logistic”
for the discrete trait. For the decision tree method, aspects,
such as tree depth used default values. The Lasso, Ridge,
and Elastic Net approaches used the package and method
glmnet, with lambda=seq The k-NN
approach used caret with k = 5 and default (equal) neighbor
weighting. The neural net used neuralnet with hidden=1,
linear.output=F. Linear discriminant analysis used the
lda package with tol=0.

5. RESULTS

were presented as AUC, accuracy, or balanced accuracy, but in
all instances higher values reﬂect better performance. Although
not all methods were represented in each study, some general
conclusions can be made. When random forests were applied,
they were either the most accurate or competitive [with the
exception of Nakano  Various
forms of neural networks often performed well, although there
is some question whether the tuning complexity is warranted. An
exception is Rousk  in
which some neural networks (perceptions) performed especially
well, but the sample size was small n = 22. In the datasets
analyzed by Ditzler et al.  the complexity and number of
nodes in neural networks showed little consistent relationship to
performance. Most of the studies used some form of higher-level
OTU aggregation, sometimes as high as the phylum level.

For the three discrete traits, we plotted one ROC curve from
each machine learning method (Figures 2A–C). The size of
each dataset (number of cases/controls X number of OTUs) is
shown in the title. Random forest (RTF) and Gradient boosted
trees (Gboost) performed well (AUC >0.85) in predicting
cases and controls in the Singh and Vincent datasets. Lasso,
ridge, elastic net (Enet), k-nearest neighbors (k-NN), and
Neural Networks (Neural) performed well in the Singh dataset
only. Generally,
linear SVM and LDA performed less well,
and SVM demonstrated close to chance performance in the
Vincent dataset.

Table 1 shows the comparative results of 17 datasets analyzed
with numerous prediction methods. The results for discrete traits

Summarizing the results after using BMI as a continuous trait
in the Goodrich dataset, the bar graph (Figure 2D) shows the

FIGURE 5 | Scatterplot comparing the average AUCs between the full dataset and the HFE subset. (A) Singh dataset, (B) Vincent dataset, (C) Goodrich dataset.

Frontiers in Genetics | www.frontiersin.org

11

June 2019 | Volume 10 | Article 579

Zhou and Gallins

Host Trait Prediction

average Pearson correlation between the predicted and actual
BMI after 100 iterations of each method. Here again the two
decision tree models performed best, although all correlations R
were <0.4.

Performance was generally poor for the Goodrich dataset,
which also included a large number of OTUs, which
presents a challenge in feature selection. We computed the
ROC curves
for each dataset after collapsing the OTUs
to the genus level (Figure 3) and after applying the HFE
method to select a subset of informative features (Figure 4).
Then we compared the AUCs between the datasets which
used all OTUs and those that used only HFE-informative
features (Figure 5).

As an overall summary, collapsing to the genus level brought
some improvement to the poorer perform prediction methods in
the Singh et al. dataset, and few other broad patterns were
apparent. In contrast, the use of cross-validated HFE produced
a great improvement in AUC in most instances (Figure 4). For
the Goodrich et al. datasets, most
methods were improved and brought to similar AUC values.
For the Vincent dataset, again most prediction methods were
improved by HFE feature-reduction, but the results were less
uniform. Another pattern that is apparent in the scatterplots,
perhaps expected,
is that HFE brought diminishing returns
for methods that already perform well. The one prediction
method that was not improved demonstrably by HFE was k-
NN (with k = 5).

6. DISCUSSION

We have presented a tutorial overview of the most commonly-
used machine learning prediction methods in microbiome host
trait prediction. Although a large number of approaches have
been used in the literature, some relative simple and clear
conclusions can be made. Decision tree methods tended to
perform well, and in the published literature similar results were
achieved by neural networks and their variants. In our analysis,
the HFE OTU feature reduction method brought a substantial
performance improvement for nearly all methods. In addition,
after such feature reduction most methods performed more
similarly. We conclude that this ﬁnding accords with the fact
that the distinction between sparse and non-sparse methods is
less dramatic after feature reduction. We hope that the tutorial,
review, and available code are useful to practitioners for host
trait prediction.

For more advanced topics, we point the reader to analysis of
microbiome time series data, using techniques, such as MDSINE
(Bucci et al., 2016), which uses dynamical systems inference to
estimate and forecast trajectories of microbiome subpopulations.
Other uses of dynamical systems have concentrated mainly on
observable phenotypes/experiemental conditions, rather than
using microbiome status for prediction (Brooks et al., 2017). In
addition, the use of co-measured features, such as metabolites
(Franzosa et al., 2019), oﬀers potentially useful
information
for integrative analyses. As another example of the use of
ancillary information, an intriguing approach has also been used
to predict biotransformation of speciﬁc drugs and xenobiotics
by gut bacterial enzymes (Sharma et al., 2017). We also
note that our review/tutorial has for clarity placed feature
engineering, which may be viewed as a form of statistical
regularization, as a separately-handled issue from the penalized
prediction modeling. Some modern sparse regression and
kernel modeling methods seek additional predictive ability by
combining feature regularization and prediction in a single step,
e.g., Xiao et al. 

AUTHOR CONTRIBUTIONS

Y-HZ is the leader of this review study. Her contribution includes
writing the manuscript, designing the data analysis, summarizing
the result, and software management. PG is responsible for
implementation of analysis, results
the manuscript writing,
summary, and code summary.

FUNDING

This work gets support from the NC State Game-changing
Research Initiative Program and CFF KNOWLE18XX0.

ACKNOWLEDGMENTS

Thanks to Mr. Chris Smith for the IT support in Bioinformatics
Research Center.

SUPPLEMENTARY MATERIAL

for this article can be found
at: https://www.frontiersin.org/articles/10.3389/fgene.

The Supplementary Material
online
2019.00579/full#supplementary-material

Supplementary Table 1 | Full table of published prediction accuracies.
