Zhou and Gallins

Host Trait Prediction

levels of taxonomy, with intriguing thoughts about the
of
interplay and use of molecular function descriptors vs. taxonomic
descriptors, the reader is referred to Knights et al. A minor fraction of missing data can often be eﬀectively
handled using simple imputation procedures, such as kNN-
impute (Crookston and Finley, 2008), or even simpler methods,
such as feature-median imputation. For the model
y = Xβ + ǫ, the training loss is Pi(yi − ˆyi)2 the most commonly-
used regularization methods are ridge regression (Hoerl and
Kennard, 1970) and Lasso (Tibshirani, 1996) regression, which
respectively use penalties λ Pi β2
i and λ Pi |βi| (not including
the intercept) to the training loss. A bias term is also added in each step,
which can be thought of as analogous to the intercept of a
linear model. for each of

For our analyses, we analyzed four traits (three binary and
one continuous) from three datasets with varying sample sizes

FIGURE 3 | ROC curves after collapsing OTUs to the genus level (A) the Singh dataset, (B) the Vincent dataset, and (C) the Goodrich dataset. Finally, we ran the
hierarchical feature engineering (HFE) algorithm introduced by
Oudah and Henschel which results fewer informative
features, including individual OTUs and aggregated elements of
the taxonomy.